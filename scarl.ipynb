{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5597867c-40fb-4eef-bf45-d4456ca7e9a3",
   "metadata": {},
   "source": [
    "# SCARL Framework Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "734501ac-6608-423e-bbe7-6402f4fd565d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCARL Framework Implementation\n",
      "==================================================\n",
      "Agent initialized with:\n",
      "  - Lambda (introspection weight): 0.3\n",
      "  - R_meta threshold: 0.5\n",
      "  - State dimension: 256\n",
      "  - Action dimension: 10\n",
      "\n",
      "Testing action selection:\n",
      "  - Selected action type: primary\n",
      "  - R_meta score: 0.522\n",
      "\n",
      "SCARL Framework ready for training!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SCARL: Self-Corrective Agentic Reinforcement Learning Framework\n",
    "Implementation based on my research paper proposing a novel RL framework\n",
    "for self-corrective NLP agents.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class ActionType(Enum):\n",
    "    \"\"\"Defines the types of actions available to the agent.\"\"\"\n",
    "    PRIMARY = \"primary\"\n",
    "    CORRECTIVE = \"corrective\"\n",
    "\n",
    "\n",
    "class CorrectiveAction(Enum):\n",
    "    \"\"\"Specific corrective actions available to the agent.\"\"\"\n",
    "    REPLAN = \"replan\"\n",
    "    SEARCH = \"search\"\n",
    "    CRITIQUE = \"critique\"\n",
    "    ASK = \"ask\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class State:\n",
    "    \"\"\"\n",
    "    Augmented state space for SCARL.\n",
    "    S_t = {S_context, S_memory, S_internal}\n",
    "    \"\"\"\n",
    "    context: Any  # Current context (e.g., conversation history, task state)\n",
    "    memory: List[Any]  # History of past actions and states\n",
    "    internal: Dict[str, Any]  # Internal reflection state\n",
    "    \n",
    "    def __init__(self, context: Any, memory: Optional[List] = None, internal: Optional[Dict] = None):\n",
    "        self.context = context\n",
    "        self.memory = memory if memory is not None else []\n",
    "        self.internal = internal if internal is not None else {\n",
    "            'r_meta': 0.0,\n",
    "            'correction_history': [],\n",
    "            'confidence': 1.0\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Action:\n",
    "    \"\"\"Represents an action with its type and content.\"\"\"\n",
    "    action_type: ActionType\n",
    "    content: Any\n",
    "    corrective_type: Optional[CorrectiveAction] = None\n",
    "\n",
    "\n",
    "class MetaRewardGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generates the intrinsic meta-reward (R_meta) signal.\n",
    "    This evaluates the quality/confidence of the agent's current trajectory.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()  # Output between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, state_embedding: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns R_meta score between 0 and 1.\n",
    "        Higher values indicate higher confidence/quality.\n",
    "        \"\"\"\n",
    "        return self.network(state_embedding)\n",
    "\n",
    "\n",
    "class PrimaryPolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    Primary policy (π_P) trained to maximize R_ext.\n",
    "    Generates task-level actions.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state_embedding: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns action probability distribution.\"\"\"\n",
    "        return self.network(state_embedding)\n",
    "    \n",
    "    def select_action(self, state_embedding: torch.Tensor) -> int:\n",
    "        \"\"\"Sample action from policy.\"\"\"\n",
    "        probs = self.forward(state_embedding)\n",
    "        action = torch.multinomial(probs, 1)\n",
    "        return action.item()\n",
    "\n",
    "\n",
    "class CorrectivePolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    Corrective policy (π_C) trained to maximize R_ext following low R_meta.\n",
    "    Selects from corrective action set: REPLAN, SEARCH, CRITIQUE, ASK.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim: int, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        num_corrective_actions = len(CorrectiveAction)\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_corrective_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state_embedding: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns corrective action probability distribution.\"\"\"\n",
    "        return self.network(state_embedding)\n",
    "    \n",
    "    def select_action(self, state_embedding: torch.Tensor) -> CorrectiveAction:\n",
    "        \"\"\"Sample corrective action from policy.\"\"\"\n",
    "        probs = self.forward(state_embedding)\n",
    "        action_idx = torch.multinomial(probs, 1).item()\n",
    "        return list(CorrectiveAction)[action_idx]\n",
    "\n",
    "\n",
    "class StateEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes the augmented state into a fixed-dimensional embedding.\n",
    "    Handles context, memory, and internal state components.\n",
    "    \"\"\"\n",
    "    def __init__(self, context_dim: int, embedding_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.context_encoder = nn.Linear(context_dim, embedding_dim // 2)\n",
    "        self.internal_encoder = nn.Linear(10, embedding_dim // 4)  # Fixed internal features\n",
    "        self.memory_encoder = nn.LSTM(context_dim, embedding_dim // 4, batch_first=True)\n",
    "        self.combiner = nn.Linear(embedding_dim, embedding_dim)\n",
    "    \n",
    "    def forward(self, state: State) -> torch.Tensor:\n",
    "        \"\"\"Encode state into embedding.\"\"\"\n",
    "        # Encode context (simplified - assumes tensor input)\n",
    "        context_emb = torch.relu(self.context_encoder(state.context))\n",
    "        \n",
    "        # Encode internal state\n",
    "        internal_features = torch.tensor([\n",
    "            state.internal.get('r_meta', 0.0),\n",
    "            state.internal.get('confidence', 1.0),\n",
    "            len(state.internal.get('correction_history', [])),\n",
    "            # Additional internal features can be added\n",
    "        ] + [0.0] * 7, dtype=torch.float32)\n",
    "        internal_emb = torch.relu(self.internal_encoder(internal_features))\n",
    "        \n",
    "        # Encode memory (simplified)\n",
    "        if state.memory:\n",
    "            memory_tensor = torch.stack(state.memory[-10:])  # Last 10 states\n",
    "            _, (memory_emb, _) = self.memory_encoder(memory_tensor.unsqueeze(0))\n",
    "            memory_emb = memory_emb.squeeze(0)\n",
    "        else:\n",
    "            memory_emb = torch.zeros(context_emb.shape[0] // 2)\n",
    "        \n",
    "        # Combine all embeddings\n",
    "        combined = torch.cat([context_emb, internal_emb, memory_emb])\n",
    "        return torch.relu(self.combiner(combined))\n",
    "\n",
    "\n",
    "class SCARLAgent:\n",
    "    \"\"\"\n",
    "    Main SCARL Agent implementing the self-corrective framework.\n",
    "    \n",
    "    Key Features:\n",
    "    - Dual policy system (primary + corrective)\n",
    "    - Meta-reward based policy switching\n",
    "    - Online introspection and error correction\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        lambda_introspection: float = 0.3,\n",
    "        r_meta_threshold: float = 0.5,\n",
    "        gamma: float = 0.99,\n",
    "        learning_rate: float = 1e-4\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize SCARL agent.\n",
    "        \n",
    "        Args:\n",
    "            state_dim: Dimension of state encoding\n",
    "            action_dim: Number of primary actions\n",
    "            lambda_introspection: Weight for meta-reward (λ in R_total = R_ext + λR_meta)\n",
    "            r_meta_threshold: Threshold for triggering corrective policy\n",
    "            gamma: Discount factor\n",
    "            learning_rate: Learning rate for all networks\n",
    "        \"\"\"\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.lambda_introspection = lambda_introspection\n",
    "        self.r_meta_threshold = r_meta_threshold\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Initialize networks\n",
    "        self.state_encoder = StateEncoder(state_dim)\n",
    "        self.meta_reward_gen = MetaRewardGenerator(state_dim)\n",
    "        self.primary_policy = PrimaryPolicy(state_dim, action_dim)\n",
    "        self.corrective_policy = CorrectivePolicy(state_dim)\n",
    "        \n",
    "        # Value networks for training (Actor-Critic)\n",
    "        self.value_network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        # Optimizers\n",
    "        self.optimizer_primary = optim.Adam(self.primary_policy.parameters(), lr=learning_rate)\n",
    "        self.optimizer_corrective = optim.Adam(self.corrective_policy.parameters(), lr=learning_rate)\n",
    "        self.optimizer_meta = optim.Adam(self.meta_reward_gen.parameters(), lr=learning_rate)\n",
    "        self.optimizer_value = optim.Adam(self.value_network.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Experience replay buffer\n",
    "        self.experience_buffer = deque(maxlen=10000)\n",
    "        \n",
    "    def select_action(self, state: State) -> Tuple[Action, float]:\n",
    "        \"\"\"\n",
    "        Main action selection with policy switching logic.\n",
    "        \n",
    "        Returns:\n",
    "            (selected_action, r_meta_score)\n",
    "        \"\"\"\n",
    "        # Encode state\n",
    "        state_emb = self.state_encoder(state)\n",
    "        \n",
    "        # Generate meta-reward\n",
    "        with torch.no_grad():\n",
    "            r_meta = self.meta_reward_gen(state_emb).item()\n",
    "        \n",
    "        # Update internal state\n",
    "        state.internal['r_meta'] = r_meta\n",
    "        \n",
    "        # Policy switching logic\n",
    "        if r_meta < self.r_meta_threshold:\n",
    "            # Trigger corrective policy\n",
    "            corrective_action = self.corrective_policy.select_action(state_emb)\n",
    "            state.internal['correction_history'].append({\n",
    "                'timestep': len(state.memory),\n",
    "                'r_meta': r_meta,\n",
    "                'action': corrective_action\n",
    "            })\n",
    "            return Action(\n",
    "                action_type=ActionType.CORRECTIVE,\n",
    "                content=corrective_action.value,\n",
    "                corrective_type=corrective_action\n",
    "            ), r_meta\n",
    "        else:\n",
    "            # Use primary policy\n",
    "            primary_action_idx = self.primary_policy.select_action(state_emb)\n",
    "            return Action(\n",
    "                action_type=ActionType.PRIMARY,\n",
    "                content=primary_action_idx,\n",
    "                corrective_type=None\n",
    "            ), r_meta\n",
    "    \n",
    "    def compute_total_reward(self, r_ext: float, r_meta: float) -> float:\n",
    "        \"\"\"\n",
    "        Compute composite reward: R_total = R_ext + λR_meta\n",
    "        \"\"\"\n",
    "        return r_ext + self.lambda_introspection * r_meta\n",
    "    \n",
    "    def store_experience(\n",
    "        self,\n",
    "        state: State,\n",
    "        action: Action,\n",
    "        r_ext: float,\n",
    "        r_meta: float,\n",
    "        next_state: State,\n",
    "        done: bool\n",
    "    ):\n",
    "        \"\"\"Store experience tuple for training.\"\"\"\n",
    "        self.experience_buffer.append({\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'r_ext': r_ext,\n",
    "            'r_meta': r_meta,\n",
    "            'next_state': next_state,\n",
    "            'done': done\n",
    "        })\n",
    "    \n",
    "    def train_step(self, batch_size: int = 32) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Perform one training step using experience replay.\n",
    "        Uses PPO-style updates for both policies.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of loss values\n",
    "        \"\"\"\n",
    "        if len(self.experience_buffer) < batch_size:\n",
    "            return {}\n",
    "        \n",
    "        # Sample batch\n",
    "        indices = np.random.choice(len(self.experience_buffer), batch_size, replace=False)\n",
    "        batch = [self.experience_buffer[i] for i in indices]\n",
    "        \n",
    "        losses = {}\n",
    "        \n",
    "        # Prepare batch data\n",
    "        states = [exp['state'] for exp in batch]\n",
    "        actions = [exp['action'] for exp in batch]\n",
    "        r_exts = torch.tensor([exp['r_ext'] for exp in batch], dtype=torch.float32)\n",
    "        r_metas = torch.tensor([exp['r_meta'] for exp in batch], dtype=torch.float32)\n",
    "        next_states = [exp['next_state'] for exp in batch]\n",
    "        dones = torch.tensor([exp['done'] for exp in batch], dtype=torch.float32)\n",
    "        \n",
    "        # Encode states\n",
    "        state_embs = torch.stack([self.state_encoder(s) for s in states])\n",
    "        next_state_embs = torch.stack([self.state_encoder(s) for s in next_states])\n",
    "        \n",
    "        # Compute values\n",
    "        values = self.value_network(state_embs).squeeze()\n",
    "        next_values = self.value_network(next_state_embs).squeeze()\n",
    "        \n",
    "        # Compute total rewards\n",
    "        r_totals = r_exts + self.lambda_introspection * r_metas\n",
    "        \n",
    "        # Compute advantages (TD error)\n",
    "        targets = r_totals + self.gamma * next_values * (1 - dones)\n",
    "        advantages = targets - values\n",
    "        \n",
    "        # Update value network\n",
    "        value_loss = nn.MSELoss()(values, targets.detach())\n",
    "        self.optimizer_value.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.optimizer_value.step()\n",
    "        losses['value_loss'] = value_loss.item()\n",
    "        \n",
    "        # Update primary policy (only for primary actions)\n",
    "        primary_mask = torch.tensor([a.action_type == ActionType.PRIMARY for a in actions])\n",
    "        if primary_mask.sum() > 0:\n",
    "            primary_state_embs = state_embs[primary_mask]\n",
    "            primary_actions = torch.tensor([\n",
    "                a.content for a in actions if a.action_type == ActionType.PRIMARY\n",
    "            ], dtype=torch.long)\n",
    "            primary_advantages = advantages[primary_mask].detach()\n",
    "            \n",
    "            action_probs = self.primary_policy(primary_state_embs)\n",
    "            log_probs = torch.log(action_probs.gather(1, primary_actions.unsqueeze(1)) + 1e-8)\n",
    "            policy_loss = -(log_probs.squeeze() * primary_advantages).mean()\n",
    "            \n",
    "            self.optimizer_primary.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.optimizer_primary.step()\n",
    "            losses['primary_policy_loss'] = policy_loss.item()\n",
    "        \n",
    "        # Update corrective policy (only for corrective actions)\n",
    "        corrective_mask = torch.tensor([a.action_type == ActionType.CORRECTIVE for a in actions])\n",
    "        if corrective_mask.sum() > 0:\n",
    "            corrective_state_embs = state_embs[corrective_mask]\n",
    "            corrective_actions = torch.tensor([\n",
    "                list(CorrectiveAction).index(a.corrective_type)\n",
    "                for a in actions if a.action_type == ActionType.CORRECTIVE\n",
    "            ], dtype=torch.long)\n",
    "            corrective_advantages = advantages[corrective_mask].detach()\n",
    "            \n",
    "            action_probs = self.corrective_policy(corrective_state_embs)\n",
    "            log_probs = torch.log(action_probs.gather(1, corrective_actions.unsqueeze(1)) + 1e-8)\n",
    "            corrective_loss = -(log_probs.squeeze() * corrective_advantages).mean()\n",
    "            \n",
    "            self.optimizer_corrective.zero_grad()\n",
    "            corrective_loss.backward()\n",
    "            self.optimizer_corrective.step()\n",
    "            losses['corrective_policy_loss'] = corrective_loss.item()\n",
    "        \n",
    "        # Update meta-reward generator to predict R_ext\n",
    "        # (Training R_meta to be predictive of future R_ext)\n",
    "        predicted_meta = self.meta_reward_gen(state_embs).squeeze()\n",
    "        meta_loss = nn.MSELoss()(predicted_meta, r_exts.detach())\n",
    "        \n",
    "        self.optimizer_meta.zero_grad()\n",
    "        meta_loss.backward()\n",
    "        self.optimizer_meta.step()\n",
    "        losses['meta_reward_loss'] = meta_loss.item()\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def set_introspection_weight(self, lambda_val: float):\n",
    "        \"\"\"Adjust the introspection weight (λ) to control caution vs. speed.\"\"\"\n",
    "        self.lambda_introspection = lambda_val\n",
    "    \n",
    "    def set_meta_threshold(self, threshold: float):\n",
    "        \"\"\"Adjust the R_meta threshold for triggering corrections.\"\"\"\n",
    "        self.r_meta_threshold = threshold\n",
    "\n",
    "\n",
    "class SCARLTrainer:\n",
    "    \"\"\"\n",
    "    Training coordinator for SCARL agent.\n",
    "    Handles episode management and training loops.\n",
    "    \"\"\"\n",
    "    def __init__(self, agent: SCARLAgent, environment: Any):\n",
    "        self.agent = agent\n",
    "        self.environment = environment\n",
    "        self.episode_rewards = []\n",
    "        self.episode_corrections = []\n",
    "    \n",
    "    def train_episode(self, max_steps: int = 100) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Run one training episode.\n",
    "        \n",
    "        Returns:\n",
    "            Episode statistics\n",
    "        \"\"\"\n",
    "        state = self.environment.reset()\n",
    "        total_r_ext = 0.0\n",
    "        total_r_meta = 0.0\n",
    "        num_corrections = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Select action\n",
    "            action, r_meta = self.agent.select_action(state)\n",
    "            \n",
    "            # Execute action in environment\n",
    "            next_state, r_ext, done, info = self.environment.step(action)\n",
    "            \n",
    "            # Track corrections\n",
    "            if action.action_type == ActionType.CORRECTIVE:\n",
    "                num_corrections += 1\n",
    "            \n",
    "            # Store experience\n",
    "            self.agent.store_experience(state, action, r_ext, r_meta, next_state, done)\n",
    "            \n",
    "            # Train agent\n",
    "            if step % 4 == 0:  # Train every 4 steps\n",
    "                self.agent.train_step(batch_size=32)\n",
    "            \n",
    "            total_r_ext += r_ext\n",
    "            total_r_meta += r_meta\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        stats = {\n",
    "            'total_r_ext': total_r_ext,\n",
    "            'avg_r_meta': total_r_meta / (step + 1),\n",
    "            'num_corrections': num_corrections,\n",
    "            'episode_length': step + 1\n",
    "        }\n",
    "        \n",
    "        self.episode_rewards.append(total_r_ext)\n",
    "        self.episode_corrections.append(num_corrections)\n",
    "        \n",
    "        return stats\n",
    "\n",
    "\n",
    "# Demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"SCARL Framework Implementation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize agent\n",
    "    agent = SCARLAgent(\n",
    "        state_dim=256,\n",
    "        action_dim=10,\n",
    "        lambda_introspection=0.3,\n",
    "        r_meta_threshold=0.5\n",
    "    )\n",
    "    \n",
    "    print(f\"Agent initialized with:\")\n",
    "    print(f\"  - Lambda (introspection weight): {agent.lambda_introspection}\")\n",
    "    print(f\"  - R_meta threshold: {agent.r_meta_threshold}\")\n",
    "    print(f\"  - State dimension: {agent.state_dim}\")\n",
    "    print(f\"  - Action dimension: {agent.action_dim}\")\n",
    "    print()\n",
    "    \n",
    "    # Create dummy state\n",
    "    dummy_context = torch.randn(256)\n",
    "    dummy_state = State(context=dummy_context)\n",
    "    \n",
    "    print(\"Testing action selection:\")\n",
    "    action, r_meta = agent.select_action(dummy_state)\n",
    "    print(f\"  - Selected action type: {action.action_type.value}\")\n",
    "    print(f\"  - R_meta score: {r_meta:.3f}\")\n",
    "    if action.action_type == ActionType.CORRECTIVE:\n",
    "        print(f\"  - Corrective action: {action.corrective_type.value}\")\n",
    "    \n",
    "    print(\"\\nSCARL Framework ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98026680-66a6-4ae4-b9ba-3d6c28b70153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCARL NLP Integration Example\n",
      "============================================================\n",
      "\n",
      "1. DIALOGUE TASK EXAMPLE\n",
      "------------------------------------------------------------\n",
      "User Query: Can you explain what Python is used for?\n",
      "Task Goal: Generate accurate and helpful response\n",
      "Loading gpt2...\n",
      "Model loaded successfully!\n",
      "\n",
      "Agent Action Type: primary\n",
      "R_meta Score: 0.505\n",
      "External Reward: 0.575\n",
      "\n",
      "------------------------------------------------------------\n",
      "To use real GPT-2 generation, create agent with:\n",
      "  agent = SCARLNLPAgent(use_real_generator=True)\n",
      "This will load GPT-2 on first text generation call.\n",
      "\n",
      "\n",
      "2. SUMMARIZATION TASK EXAMPLE\n",
      "------------------------------------------------------------\n",
      "Source Text Length: 321 characters\n",
      "Task: Generate accurate summary\n",
      "\n",
      "R_meta Score: 0.506\n",
      "Action Type: primary\n",
      "\n",
      "============================================================\n",
      "Integration complete! Ready for training.\n",
      "\n",
      "Next steps:\n",
      "1. Replace text generation placeholder with actual LLM (GPT, T5, etc.)\n",
      "2. Implement proper reward models for task evaluation\n",
      "3. Train meta-reward generator on diverse NLP tasks\n",
      "4. Collect self-correction trajectory data for training\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SCARL Integration with NLP Environments\n",
    "Demonstrates how to integrate the SCARL framework with real NLP tasks\n",
    "using language models and text-based environments.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import re\n",
    "\n",
    "\n",
    "class NLPStateEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes NLP-specific state information using a pre-trained language model.\n",
    "    Handles text context, conversation history, and internal agent state.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"bert-base-uncased\", embedding_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.language_model = AutoModel.from_pretrained(model_name)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Freeze LM parameters for efficiency (optional)\n",
    "        for param in self.language_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        lm_hidden_size = self.language_model.config.hidden_size\n",
    "        \n",
    "        # Project LM outputs to embedding dimension\n",
    "        self.context_projection = nn.Linear(lm_hidden_size, embedding_dim // 2)\n",
    "        self.history_lstm = nn.LSTM(lm_hidden_size, embedding_dim // 4, batch_first=True)\n",
    "        self.internal_encoder = nn.Linear(10, embedding_dim // 4)\n",
    "        self.combiner = nn.Linear(embedding_dim, embedding_dim)\n",
    "    \n",
    "    def encode_text(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"Encode text using language model. Returns shape [hidden_size]\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.language_model(**inputs)\n",
    "        # Return [CLS] token embedding, squeeze to 1D\n",
    "        return outputs.last_hidden_state[0, 0, :]  # Shape: [hidden_size]\n",
    "    \n",
    "    def forward(self, state: 'NLPState') -> torch.Tensor:\n",
    "        \"\"\"Encode NLP state into fixed-dimensional embedding. Returns shape [embedding_dim]\"\"\"\n",
    "        # Encode current context - shape: [hidden_size]\n",
    "        context_emb = self.encode_text(state.context_text)\n",
    "        # Project to embedding_dim // 2 - shape: [embedding_dim // 2]\n",
    "        context_emb = torch.relu(self.context_projection(context_emb))\n",
    "        \n",
    "        # Encode conversation history\n",
    "        if state.history and len(state.history) > 0:\n",
    "            history_texts = state.history[-5:]  # Last 5 turns\n",
    "            # Stack embeddings - shape: [num_turns, hidden_size]\n",
    "            history_embs = torch.stack([self.encode_text(turn) for turn in history_texts])\n",
    "            # LSTM expects [batch, seq, features] - add batch dim\n",
    "            # Shape: [1, num_turns, hidden_size]\n",
    "            _, (history_emb, _) = self.history_lstm(history_embs.unsqueeze(0))\n",
    "            # Shape: [1, embedding_dim // 4] -> squeeze -> [embedding_dim // 4]\n",
    "            history_emb = history_emb.squeeze(0).squeeze(0)\n",
    "        else:\n",
    "            # Create zero embedding - shape: [embedding_dim // 4]\n",
    "            history_emb = torch.zeros(self.embedding_dim // 4)\n",
    "        \n",
    "        # Encode internal state - shape: [10]\n",
    "        internal_features = torch.tensor([\n",
    "            state.internal.get('r_meta', 0.0),\n",
    "            state.internal.get('confidence', 1.0),\n",
    "            len(state.internal.get('correction_history', [])),\n",
    "            float(state.internal.get('last_correction_successful', False)),\n",
    "            state.internal.get('turns_since_correction', 0),\n",
    "        ] + [0.0] * 5, dtype=torch.float32)\n",
    "        # Project to embedding_dim // 4 - shape: [embedding_dim // 4]\n",
    "        internal_emb = torch.relu(self.internal_encoder(internal_features))\n",
    "        \n",
    "        # Combine all embeddings - all should be 1D tensors now\n",
    "        # Shapes: [embedding_dim // 2] + [embedding_dim // 4] + [embedding_dim // 4] = [embedding_dim]\n",
    "        combined = torch.cat([context_emb, internal_emb, history_emb])\n",
    "        \n",
    "        # Final projection - input: [embedding_dim], output: [embedding_dim]\n",
    "        return torch.relu(self.combiner(combined))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NLPState:\n",
    "    \"\"\"\n",
    "    NLP-specific state representation.\n",
    "    Extends the base State class with text-specific fields.\n",
    "    \"\"\"\n",
    "    context_text: str  # Current text context\n",
    "    history: List[str]  # Conversation/interaction history\n",
    "    task_goal: str  # Task description\n",
    "    generated_text: str  # Text generated so far\n",
    "    internal: Dict[str, Any]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        context_text: str,\n",
    "        task_goal: str,\n",
    "        history: Optional[List[str]] = None,\n",
    "        generated_text: str = \"\",\n",
    "        internal: Optional[Dict] = None\n",
    "    ):\n",
    "        self.context_text = context_text\n",
    "        self.task_goal = task_goal\n",
    "        self.history = history if history is not None else []\n",
    "        self.generated_text = generated_text\n",
    "        self.internal = internal if internal is not None else {\n",
    "            'r_meta': 0.0,\n",
    "            'correction_history': [],\n",
    "            'confidence': 1.0,\n",
    "            'last_correction_successful': False,\n",
    "            'turns_since_correction': 0\n",
    "        }\n",
    "\n",
    "\n",
    "class NLPEnvironment:\n",
    "    \"\"\"\n",
    "    Base class for NLP environments.\n",
    "    Defines the interface for text-based RL tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, task_description: str):\n",
    "        self.task_description = task_description\n",
    "        self.current_state = None\n",
    "        self.episode_history = []\n",
    "    \n",
    "    def reset(self) -> NLPState:\n",
    "        \"\"\"Reset environment and return initial state.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def step(self, action: Any) -> Tuple[NLPState, float, bool, Dict]:\n",
    "        \"\"\"\n",
    "        Execute action and return (next_state, reward, done, info).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Display current state (optional).\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class DialogueEnvironment(NLPEnvironment):\n",
    "    \"\"\"\n",
    "    Environment for dialogue/conversation tasks.\n",
    "    Goal: Generate helpful, accurate, and safe responses.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_query: str,\n",
    "        knowledge_base: Optional[Dict[str, str]] = None,\n",
    "        max_turns: int = 10\n",
    "    ):\n",
    "        super().__init__(\"Generate helpful dialogue response\")\n",
    "        self.user_query = user_query\n",
    "        self.knowledge_base = knowledge_base or {}\n",
    "        self.max_turns = max_turns\n",
    "        self.turn_count = 0\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def reset(self) -> NLPState:\n",
    "        \"\"\"Initialize dialogue session.\"\"\"\n",
    "        self.turn_count = 0\n",
    "        self.conversation_history = [f\"User: {self.user_query}\"]\n",
    "        \n",
    "        return NLPState(\n",
    "            context_text=self.user_query,\n",
    "            task_goal=\"Generate accurate and helpful response\",\n",
    "            history=self.conversation_history.copy(),\n",
    "            generated_text=\"\"\n",
    "        )\n",
    "    \n",
    "    def step(self, action: Any) -> Tuple[NLPState, float, bool, Dict]:\n",
    "        \"\"\"Process agent's action (text generation or correction).\"\"\"\n",
    "        \n",
    "        self.turn_count += 1\n",
    "        info = {}\n",
    "        r_ext = 0.0\n",
    "        \n",
    "        if isinstance(action, Action):\n",
    "            if action.action_type == ActionType.PRIMARY:\n",
    "                # Primary action: generate text\n",
    "                generated_text = action.content\n",
    "                self.conversation_history.append(f\"Agent: {generated_text}\")\n",
    "                \n",
    "                # Compute reward based on response quality\n",
    "                r_ext = self._evaluate_response(generated_text)\n",
    "                info['response'] = generated_text\n",
    "                \n",
    "            elif action.action_type == ActionType.CORRECTIVE:\n",
    "                # Corrective action: modify approach\n",
    "                if action.corrective_type == CorrectiveAction.SEARCH:\n",
    "                    # Agent searches knowledge base\n",
    "                    search_results = self._search_knowledge_base(self.user_query)\n",
    "                    info['search_results'] = search_results\n",
    "                    r_ext = 0.1  # Small reward for seeking information\n",
    "                    \n",
    "                elif action.corrective_type == CorrectiveAction.REPLAN:\n",
    "                    # Agent replans approach\n",
    "                    info['action'] = 'replanning'\n",
    "                    r_ext = 0.0  # Neutral reward\n",
    "                    \n",
    "                elif action.corrective_type == CorrectiveAction.CRITIQUE:\n",
    "                    # Agent critiques own response\n",
    "                    critique = action.content\n",
    "                    info['critique'] = critique\n",
    "                    r_ext = 0.05  # Small reward for self-reflection\n",
    "                    \n",
    "                elif action.corrective_type == CorrectiveAction.ASK:\n",
    "                    # Agent asks clarifying question\n",
    "                    clarification = action.content\n",
    "                    info['clarification_request'] = clarification\n",
    "                    r_ext = 0.15  # Reward for seeking clarity\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = self.turn_count >= self.max_turns\n",
    "        \n",
    "        # Create next state\n",
    "        next_state = NLPState(\n",
    "            context_text=self.conversation_history[-1] if self.conversation_history else \"\",\n",
    "            task_goal=self.task_description,\n",
    "            history=self.conversation_history.copy(),\n",
    "            generated_text=info.get('response', '')\n",
    "        )\n",
    "        \n",
    "        return next_state, r_ext, done, info\n",
    "    \n",
    "    def _evaluate_response(self, response: str) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate response quality (R_ext).\n",
    "        In practice, this would use human feedback or a reward model.\n",
    "        \"\"\"\n",
    "        reward = 0.0\n",
    "        \n",
    "        # Simple heuristics (replace with actual reward model)\n",
    "        if len(response) > 10:  # Non-trivial response\n",
    "            reward += 0.3\n",
    "        \n",
    "        if len(response) < 500:  # Not too verbose\n",
    "            reward += 0.2\n",
    "        \n",
    "        # Check if response addresses the query\n",
    "        query_words = set(self.user_query.lower().split())\n",
    "        response_words = set(response.lower().split())\n",
    "        overlap = len(query_words & response_words) / max(len(query_words), 1)\n",
    "        reward += overlap * 0.3\n",
    "        \n",
    "        # Penalize unsafe content (simple check)\n",
    "        unsafe_patterns = ['hack', 'steal', 'illegal']\n",
    "        if any(pattern in response.lower() for pattern in unsafe_patterns):\n",
    "            reward -= 0.5\n",
    "        \n",
    "        return np.clip(reward, -1.0, 1.0)\n",
    "    \n",
    "    def _search_knowledge_base(self, query: str) -> str:\n",
    "        \"\"\"Search knowledge base for relevant information.\"\"\"\n",
    "        # Simple keyword matching\n",
    "        for key, value in self.knowledge_base.items():\n",
    "            if key.lower() in query.lower():\n",
    "                return value\n",
    "        return \"No relevant information found.\"\n",
    "\n",
    "\n",
    "class SummarizationEnvironment(NLPEnvironment):\n",
    "    \"\"\"\n",
    "    Environment for text summarization tasks.\n",
    "    Goal: Generate accurate, concise summaries.\n",
    "    \"\"\"\n",
    "    def __init__(self, source_text: str, max_summary_length: int = 150):\n",
    "        super().__init__(\"Generate concise and accurate summary\")\n",
    "        self.source_text = source_text\n",
    "        self.max_summary_length = max_summary_length\n",
    "        self.current_summary = \"\"\n",
    "    \n",
    "    def reset(self) -> NLPState:\n",
    "        \"\"\"Initialize summarization task.\"\"\"\n",
    "        self.current_summary = \"\"\n",
    "        \n",
    "        return NLPState(\n",
    "            context_text=self.source_text,\n",
    "            task_goal=\"Generate accurate summary\",\n",
    "            history=[],\n",
    "            generated_text=\"\"\n",
    "        )\n",
    "    \n",
    "    def step(self, action: Any) -> Tuple[NLPState, float, bool, Dict]:\n",
    "        \"\"\"Process summarization action.\"\"\"\n",
    "        \n",
    "        info = {}\n",
    "        done = False\n",
    "        r_ext = 0.0\n",
    "        \n",
    "        if isinstance(action, Action):\n",
    "            if action.action_type == ActionType.PRIMARY:\n",
    "                # Generate or extend summary\n",
    "                summary_text = action.content\n",
    "                self.current_summary = summary_text\n",
    "                \n",
    "                # Evaluate summary quality\n",
    "                r_ext = self._evaluate_summary(summary_text)\n",
    "                info['summary'] = summary_text\n",
    "                done = True  # Single-step task\n",
    "                \n",
    "            elif action.action_type == ActionType.CORRECTIVE:\n",
    "                if action.corrective_type == CorrectiveAction.REPLAN:\n",
    "                    # Start over with new approach\n",
    "                    self.current_summary = \"\"\n",
    "                    r_ext = 0.0\n",
    "                    \n",
    "                elif action.corrective_type == CorrectiveAction.CRITIQUE:\n",
    "                    # Self-critique current summary\n",
    "                    critique = self._generate_critique()\n",
    "                    info['critique'] = critique\n",
    "                    r_ext = 0.1\n",
    "        \n",
    "        next_state = NLPState(\n",
    "            context_text=self.source_text,\n",
    "            task_goal=self.task_description,\n",
    "            history=[self.current_summary] if self.current_summary else [],\n",
    "            generated_text=self.current_summary\n",
    "        )\n",
    "        \n",
    "        return next_state, r_ext, done, info\n",
    "    \n",
    "    def _evaluate_summary(self, summary: str) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate summary quality using ROUGE-like heuristics.\n",
    "        In practice, use actual ROUGE or learned reward model.\n",
    "        \"\"\"\n",
    "        reward = 0.0\n",
    "        \n",
    "        # Length penalty\n",
    "        if len(summary) <= self.max_summary_length:\n",
    "            reward += 0.3\n",
    "        else:\n",
    "            reward -= 0.2\n",
    "        \n",
    "        # Coverage: check if key concepts are included\n",
    "        source_words = set(self.source_text.lower().split())\n",
    "        summary_words = set(summary.lower().split())\n",
    "        \n",
    "        # Simple n-gram overlap (simplified ROUGE)\n",
    "        overlap = len(source_words & summary_words) / max(len(source_words), 1)\n",
    "        reward += overlap * 0.7\n",
    "        \n",
    "        return np.clip(reward, -1.0, 1.0)\n",
    "    \n",
    "    def _generate_critique(self) -> str:\n",
    "        \"\"\"Generate self-critique of current summary.\"\"\"\n",
    "        if not self.current_summary:\n",
    "            return \"Summary is empty. Need to generate content.\"\n",
    "        if len(self.current_summary) > self.max_summary_length:\n",
    "            return \"Summary is too long. Need to condense.\"\n",
    "        return \"Summary structure looks reasonable. Check for key details.\"\n",
    "\n",
    "\n",
    "class NLPMetaRewardGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    NLP-specific meta-reward generator.\n",
    "    Evaluates text quality, coherence, and safety.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, state_embedding: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns R_meta score for text generation quality.\n",
    "        Expects input shape: [state_dim]\n",
    "        Returns shape: [1] (scalar)\n",
    "        \"\"\"\n",
    "        # Ensure input is 1D, add batch dimension\n",
    "        if state_embedding.dim() == 1:\n",
    "            state_embedding = state_embedding.unsqueeze(0)  # [1, state_dim]\n",
    "        \n",
    "        output = self.network(state_embedding)  # [1, 1]\n",
    "        return output.squeeze()  # scalar\n",
    "\n",
    "\n",
    "class TextGeneratorWrapper:\n",
    "    \"\"\"\n",
    "    Wrapper for text generation models (GPT-2, T5, etc.)\n",
    "    Lazy loads the model only when first used.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"gpt2\"):\n",
    "        self.model_name = model_name\n",
    "        self._model = None\n",
    "        self._tokenizer = None\n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"Lazy load the model and tokenizer.\"\"\"\n",
    "        if self._model is None:\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "            print(f\"Loading {self.model_name}...\")\n",
    "            self._model = AutoModelForCausalLM.from_pretrained(self.model_name)\n",
    "            self._tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            self._tokenizer.pad_token = self._tokenizer.eos_token\n",
    "            print(\"Model loaded successfully!\")\n",
    "    \n",
    "    def generate(self, prompt: str, max_new_tokens: int = 100, temperature: float = 0.7) -> str:\n",
    "        \"\"\"Generate text from prompt.\"\"\"\n",
    "        self.load()\n",
    "        \n",
    "        inputs = self._tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        outputs = self._model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self._tokenizer.eos_token_id,\n",
    "            attention_mask=inputs.attention_mask\n",
    "        )\n",
    "        \n",
    "        generated = self._tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Remove the prompt from the output\n",
    "        if generated.startswith(prompt):\n",
    "            generated = generated[len(prompt):].strip()\n",
    "        \n",
    "        return generated\n",
    "\n",
    "\n",
    "class SCARLNLPAgent:\n",
    "    \"\"\"\n",
    "    SCARL agent specifically configured for NLP tasks.\n",
    "    Integrates language model with SCARL's self-correction mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"bert-base-uncased\",\n",
    "        embedding_dim: int = 256,\n",
    "        lambda_introspection: float = 0.3,\n",
    "        r_meta_threshold: float = 0.5,\n",
    "        use_real_generator: bool = False,\n",
    "        generator_model: str = \"gpt2\"\n",
    "    ):\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lambda_introspection = lambda_introspection\n",
    "        self.r_meta_threshold = r_meta_threshold\n",
    "        self.use_real_generator = use_real_generator\n",
    "        \n",
    "        # NLP-specific components\n",
    "        self.state_encoder = NLPStateEncoder(model_name, embedding_dim)\n",
    "        self.meta_reward_gen = NLPMetaRewardGenerator(embedding_dim)\n",
    "        \n",
    "        # Policies (using base SCARL policies)\n",
    "        self.primary_policy = PrimaryPolicy(embedding_dim, action_dim=256)\n",
    "        self.corrective_policy = CorrectivePolicy(embedding_dim)\n",
    "        \n",
    "        # Text generation model (optional)\n",
    "        if use_real_generator:\n",
    "            self.text_generator = TextGeneratorWrapper(generator_model)\n",
    "        else:\n",
    "            self.text_generator = None\n",
    "    \n",
    "    def select_action(self, state: NLPState) -> Tuple[Any, float]:\n",
    "        \"\"\"Select action based on NLP state.\"\"\"\n",
    "        \n",
    "        # Encode state - returns 1D tensor [embedding_dim]\n",
    "        state_emb = self.state_encoder(state)\n",
    "        \n",
    "        # Generate meta-reward - input is 1D, output is scalar\n",
    "        with torch.no_grad():\n",
    "            r_meta_tensor = self.meta_reward_gen(state_emb)\n",
    "            # Ensure it's a scalar\n",
    "            if r_meta_tensor.dim() > 0:\n",
    "                r_meta = r_meta_tensor.item()\n",
    "            else:\n",
    "                r_meta = float(r_meta_tensor)\n",
    "        \n",
    "        # Update internal state\n",
    "        state.internal['r_meta'] = r_meta\n",
    "        state.internal['turns_since_correction'] += 1\n",
    "        \n",
    "        # Policy switching\n",
    "        if r_meta < self.r_meta_threshold:\n",
    "            # Low confidence - trigger correction\n",
    "            corrective_action = self.corrective_policy.select_action(state_emb.unsqueeze(0))\n",
    "            state.internal['correction_history'].append({\n",
    "                'turn': len(state.history),\n",
    "                'r_meta': r_meta,\n",
    "                'action': corrective_action.value\n",
    "            })\n",
    "            state.internal['turns_since_correction'] = 0\n",
    "            \n",
    "            return Action(\n",
    "                action_type=ActionType.CORRECTIVE,\n",
    "                content=self._execute_corrective_action(corrective_action, state),\n",
    "                corrective_type=corrective_action\n",
    "            ), r_meta\n",
    "        else:\n",
    "            # High confidence - proceed with primary action\n",
    "            generated_text = self._generate_text(state)\n",
    "            return Action(\n",
    "                action_type=ActionType.PRIMARY,\n",
    "                content=generated_text,\n",
    "                corrective_type=None\n",
    "            ), r_meta\n",
    "    \n",
    "    def _generate_text(self, state: NLPState) -> str:\n",
    "        \"\"\"\n",
    "        Generate text using language model.\n",
    "        Uses real LLM if configured, otherwise returns placeholder.\n",
    "        \"\"\"\n",
    "        if self.use_real_generator and self.text_generator:\n",
    "            prompt = f\"Context: {state.context_text}\\n\\nProvide a helpful response:\"\n",
    "            return self.text_generator.generate(prompt, max_new_tokens=100)\n",
    "        else:\n",
    "            # Placeholder for demo\n",
    "            return f\"Generated response based on: {state.context_text[:50]}...\"\n",
    "    \n",
    "    def _execute_corrective_action(\n",
    "        self,\n",
    "        action: 'CorrectiveAction',\n",
    "        state: NLPState\n",
    "    ) -> str:\n",
    "        \"\"\"Execute specific corrective action.\"\"\"        \n",
    "        if action == CorrectiveAction.SEARCH:\n",
    "            return f\"Searching for information about: {state.task_goal}\"\n",
    "        elif action == CorrectiveAction.REPLAN:\n",
    "            return \"Replanning approach to task\"\n",
    "        elif action == CorrectiveAction.CRITIQUE:\n",
    "            return f\"Critiquing current approach: {state.generated_text[:50]}...\"\n",
    "        elif action == CorrectiveAction.ASK:\n",
    "            return \"Could you provide more details about what you're looking for?\"\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Example integration demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"SCARL NLP Integration Example\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Setup 1: Dialogue Environment\n",
    "    print(\"\\n1. DIALOGUE TASK EXAMPLE\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    knowledge_base = {\n",
    "        \"python\": \"Python is a high-level programming language known for simplicity.\",\n",
    "        \"machine learning\": \"ML is a subset of AI focused on learning from data.\"\n",
    "    }\n",
    "    \n",
    "    dialogue_env = DialogueEnvironment(\n",
    "        user_query=\"Can you explain what Python is used for?\",\n",
    "        knowledge_base=knowledge_base,\n",
    "        max_turns=5\n",
    "    )\n",
    "    \n",
    "    agent = SCARLNLPAgent(\n",
    "        lambda_introspection=0.3,\n",
    "        r_meta_threshold=0.5,\n",
    "        use_real_generator=True\n",
    "    )\n",
    "    \n",
    "    state = dialogue_env.reset()\n",
    "    print(f\"User Query: {state.context_text}\")\n",
    "    print(f\"Task Goal: {state.task_goal}\")\n",
    "    \n",
    "    # Simulate one interaction\n",
    "    action, r_meta = agent.select_action(state)\n",
    "    next_state, reward, done, info = dialogue_env.step(action)\n",
    "    \n",
    "    print(f\"\\nAgent Action Type: {action.action_type.value}\")\n",
    "    print(f\"R_meta Score: {r_meta:.3f}\")\n",
    "    print(f\"External Reward: {reward:.3f}\")\n",
    "    if action.corrective_type:\n",
    "        print(f\"Corrective Action: {action.corrective_type.value}\")\n",
    "\n",
    "    \n",
    "    # Setup 2: Summarization Environment\n",
    "    print(\"\\n\\n2. SUMMARIZATION TASK EXAMPLE\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    source_text = \"\"\"\n",
    "    Artificial intelligence has revolutionized many industries. Machine learning,\n",
    "    a subset of AI, enables computers to learn from data without explicit programming.\n",
    "    Deep learning, using neural networks, has achieved remarkable results in image\n",
    "    recognition, natural language processing, and game playing.\n",
    "    \"\"\"\n",
    "    \n",
    "    summ_env = SummarizationEnvironment(\n",
    "        source_text=source_text,\n",
    "        max_summary_length=100\n",
    "    )\n",
    "    \n",
    "    state = summ_env.reset()\n",
    "    print(f\"Source Text Length: {len(source_text)} characters\")\n",
    "    print(f\"Task: {state.task_goal}\")\n",
    "    \n",
    "    action, r_meta = agent.select_action(state)\n",
    "    print(f\"\\nR_meta Score: {r_meta:.3f}\")\n",
    "    print(f\"Action Type: {action.action_type.value}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Integration complete! Ready for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8872e240-c564-42ee-8af5-76127f6a94e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
